{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "0kMunYYNNJr4",
        "outputId": "e5530ded-0b53-47b4-c9c9-d73ab44ae0c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nltk-3.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              },
              "id": "7c0ab1c00f44441982b126205ba4bc3f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK fallback\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN7ihhn0MOff",
        "outputId": "3fea03b0-1c2c-4f47-f15f-7b6ef9d4c417"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — imports and helper functions\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# NLP libraries\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "# sklearn for TF-IDF + similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Load spaCy model (we'll try/catch)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    SPACY_OK = True\n",
        "except Exception as e:\n",
        "    print(\"spaCy model not available.\", e)\n",
        "    nlp = None\n",
        "    SPACY_OK = False"
      ],
      "metadata": {
        "id": "mogTYtsCLv-8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EN_STOPWORDS = set(stopwords.words('english'))\n",
        "N_LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "wYrroNBWMhIy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — cleaning + lemmatization (spaCy primary, NLTK fallback)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Light cleaning: remove emails/urls, keep alphanumerics, allow + and # (for C++ / C#).\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)          # remove emails\n",
        "    text = re.sub(r'http\\S+', ' ', text)          # remove urls\n",
        "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\+\\#\\.\\- ]+', ' ', text)  # allow +,#,.,-\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def lemmatize_tokens(text: str, use_spacy: bool = True) -> list:\n",
        "    \"\"\"Return lowercased, lemmatized tokens with stopwords removed.\"\"\"\n",
        "    text = clean_text(text)\n",
        "    if use_spacy and SPACY_OK:\n",
        "        doc = nlp(text)\n",
        "        tokens = [token.lemma_.lower() for token in doc\n",
        "                  if token.is_alpha and not token.is_stop]\n",
        "        return tokens\n",
        "    # Fallback: NLTK\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t.lower() for t in tokens if t.isalpha() and t.lower() not in EN_STOPWORDS]\n",
        "    lem = [N_LEMMATIZER.lemmatize(t) for t in tokens]\n",
        "    return lem\n"
      ],
      "metadata": {
        "id": "9SVURM1XMSCk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 — extract named entities and noun-chunks (useful to detect company names, technologies, etc.)\n",
        "\n",
        "def extract_entities(text: str):\n",
        "    \"\"\"Return spaCy entities as list of (text, label). If spaCy not available, returns empty list.\"\"\"\n",
        "    if not SPACY_OK:\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "def extract_noun_phrases(text: str):\n",
        "    \"\"\"Return noun-chunks / proper nouns (good for multi-word skills like 'machine learning').\"\"\"\n",
        "    if SPACY_OK:\n",
        "        doc = nlp(text)\n",
        "        chunks = [chunk.text.lower().strip() for chunk in doc.noun_chunks]\n",
        "        # also include PROPN tokens (single-word proper nouns)\n",
        "        props = [token.text.lower() for token in doc if token.pos_ == \"PROPN\"]\n",
        "        return list(set(chunks + props))\n",
        "    # Fallback: simple heuristic - return lowercased bigrams/trigrams from tokens\n",
        "    tokens = [t.lower() for t in word_tokenize(clean_text(text)) if t.isalpha()]\n",
        "    bigrams = [\" \".join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
        "    trigrams = [\" \".join(tokens[i:i+3]) for i in range(len(tokens)-2)]\n",
        "    return list(set(bigrams + trigrams))\n"
      ],
      "metadata": {
        "id": "NkRHpnM-NuhB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 — prepare a sample skill list and a phrase matcher for robust skill extraction\n",
        "\n",
        "# --------- SAMPLE skill list (expand this into a master file or DB later) ----------\n",
        "skills_master = [\n",
        "    \"python\", \"java\", \"javascript\", \"react\", \"node.js\", \"node\", \"express\",\n",
        "    \"flask\", \"django\", \"fastapi\", \"sql\", \"postgresql\", \"mysql\",\n",
        "    \"mongodb\", \"aws\", \"azure\", \"google cloud\", \"docker\", \"kubernetes\",\n",
        "    \"machine learning\", \"deep learning\", \"nlp\", \"tensorflow\", \"pytorch\",\n",
        "    \"git\", \"rest api\", \"restful api\", \"html\", \"css\", \"typescript\", \"c++\", \"c#\"\n",
        "]\n",
        "# normalize skills for phrase matcher\n",
        "skills_master = list(dict.fromkeys(sk.lower() for sk in skills_master))  # dedupe & lower\n",
        "\n",
        "# If spaCy is available make a PhraseMatcher for exact phrase matches (fast + accurate)\n",
        "if SPACY_OK:\n",
        "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "    patterns = [nlp.make_doc(s) for s in skills_master]\n",
        "    matcher.add(\"SKILLS\", patterns)\n",
        "else:\n",
        "    matcher = None\n",
        "\n",
        "def extract_skills(text: str, skills_list=skills_master) -> list:\n",
        "    \"\"\"Return matched skills from text using spaCy PhraseMatcher or substring fallback.\"\"\"\n",
        "    text_clean = clean_text(text).lower()\n",
        "    found = set()\n",
        "    if SPACY_OK and matcher is not None:\n",
        "        doc = nlp(text_clean)\n",
        "        matches = matcher(doc)\n",
        "        for match_id, start, end in matches:\n",
        "            found.add(doc[start:end].text.lower())\n",
        "        return sorted(found)\n",
        "    # Fallback: simple substring matching (order by longest match)\n",
        "    for skill in sorted(skills_list, key=lambda s: -len(s)):\n",
        "        if skill in text_clean:\n",
        "            found.add(skill)\n",
        "    return sorted(found)\n"
      ],
      "metadata": {
        "id": "gqRXB_0XN0_T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6 — TF-IDF similarity (same idea as Day 1)\n",
        "def tfidf_similarity(text_a: str, text_b: str) -> float:\n",
        "    vec = TfidfVectorizer().fit([text_a, text_b])\n",
        "    tfidf = vec.transform([text_a, text_b])\n",
        "    sim = cosine_similarity(tfidf[0], tfidf[1])[0,0]\n",
        "    return float(sim)\n",
        "\n",
        "# OPTIONAL: sentence-transformers embeddings (for Day 3 semantic matching)\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# def embed_similarity(a,b):\n",
        "#     a_emb = model.encode([a])\n",
        "#     b_emb = model.encode([b])\n",
        "#     return cosine_similarity(a_emb, b_emb)[0,0]\n"
      ],
      "metadata": {
        "id": "f57__7KuN99j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 — main function to produce structured JSON result for a resume vs JD pair\n",
        "\n",
        "def match_resume_jd(resume_text: str, jd_text: str, skills_list=skills_master, use_spacy_lemmatize=True):\n",
        "    # Clean / lemmatize\n",
        "    cleaned_resume = clean_text(resume_text)\n",
        "    cleaned_jd = clean_text(jd_text)\n",
        "    resume_tokens = lemmatize_tokens(cleaned_resume, use_spacy=use_spacy_lemmatize)\n",
        "    jd_tokens = lemmatize_tokens(cleaned_jd, use_spacy=use_spacy_lemmatize)\n",
        "\n",
        "    # Skill extraction\n",
        "    skills_resume = set(extract_skills(resume_text, skills_list))\n",
        "    skills_jd = set(extract_skills(jd_text, skills_list))\n",
        "    matched_skills = sorted(list(skills_resume & skills_jd))\n",
        "    missing_skills = sorted(list(skills_jd - skills_resume))  # skills JD expects but resume missing\n",
        "\n",
        "    # Entity extraction\n",
        "    entities_resume = extract_entities(resume_text)\n",
        "    entities_jd = extract_entities(jd_text)\n",
        "\n",
        "    # Similarities\n",
        "    tfidf_score = tfidf_similarity(cleaned_resume, cleaned_jd)\n",
        "\n",
        "    result = {\n",
        "        \"matched_skills\": matched_skills,\n",
        "        \"missing_skills\": missing_skills,\n",
        "        \"skills_found_in_resume\": sorted(list(skills_resume)),\n",
        "        \"skills_found_in_jd\": sorted(list(skills_jd)),\n",
        "        \"tfidf_score\": round(tfidf_score, 4),\n",
        "        \"entities_resume\": entities_resume,\n",
        "        \"entities_jd\": entities_jd,\n",
        "        \"lemmatized_resume_tokens_preview\": resume_tokens[:60],\n",
        "        \"lemmatized_jd_tokens_preview\": jd_tokens[:60],\n",
        "    }\n",
        "    return result\n",
        "\n",
        "# Save results to JSON utility\n",
        "def save_result_json(result: dict, filename: str):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Saved: {filename}\")\n"
      ],
      "metadata": {
        "id": "5ylyww2dN97o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8 — Example usage\n",
        "sample_resume = \"\"\"\n",
        "Experienced backend engineer. Built RESTful APIs with Flask and Django, worked with PostgreSQL and MongoDB.\n",
        "Familiar with Docker and AWS. Good knowledge of Python, unit testing, and Git.\n",
        "\"\"\"\n",
        "\n",
        "sample_jd = \"\"\"\n",
        "Hiring Backend Developer: must have Python experience (Flask or FastAPI), PostgreSQL or MySQL,\n",
        "containerization with Docker, cloud experience (AWS preferred). Experience in Kubernetes is a plus.\n",
        "\"\"\"\n",
        "\n",
        "res = match_resume_jd(sample_resume, sample_jd)\n",
        "print(json.dumps(res, indent=2))\n",
        "\n",
        "# Save result\n",
        "save_result_json(res, \"sample_resume1_vs_jd1.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V65FjH0sOJoj",
        "outputId": "6cbce43a-1ac2-4821-f4a0-4c1b80d85534"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"matched_skills\": [\n",
            "    \"aws\",\n",
            "    \"docker\",\n",
            "    \"flask\",\n",
            "    \"postgresql\",\n",
            "    \"python\"\n",
            "  ],\n",
            "  \"missing_skills\": [\n",
            "    \"fastapi\",\n",
            "    \"kubernetes\",\n",
            "    \"mysql\"\n",
            "  ],\n",
            "  \"skills_found_in_resume\": [\n",
            "    \"aws\",\n",
            "    \"django\",\n",
            "    \"docker\",\n",
            "    \"flask\",\n",
            "    \"git\",\n",
            "    \"mongodb\",\n",
            "    \"postgresql\",\n",
            "    \"python\"\n",
            "  ],\n",
            "  \"skills_found_in_jd\": [\n",
            "    \"aws\",\n",
            "    \"docker\",\n",
            "    \"fastapi\",\n",
            "    \"flask\",\n",
            "    \"kubernetes\",\n",
            "    \"mysql\",\n",
            "    \"postgresql\",\n",
            "    \"python\"\n",
            "  ],\n",
            "  \"tfidf_score\": 0.1349,\n",
            "  \"entities_resume\": [\n",
            "    [\n",
            "      \"Flask\",\n",
            "      \"GPE\"\n",
            "    ],\n",
            "    [\n",
            "      \"Django\",\n",
            "      \"GPE\"\n",
            "    ],\n",
            "    [\n",
            "      \"PostgreSQL\",\n",
            "      \"GPE\"\n",
            "    ],\n",
            "    [\n",
            "      \"Docker\",\n",
            "      \"PERSON\"\n",
            "    ],\n",
            "    [\n",
            "      \"AWS\",\n",
            "      \"ORG\"\n",
            "    ],\n",
            "    [\n",
            "      \"Python\",\n",
            "      \"ORG\"\n",
            "    ],\n",
            "    [\n",
            "      \"Git\",\n",
            "      \"PERSON\"\n",
            "    ]\n",
            "  ],\n",
            "  \"entities_jd\": [\n",
            "    [\n",
            "      \"Flask\",\n",
            "      \"PRODUCT\"\n",
            "    ],\n",
            "    [\n",
            "      \"PostgreSQL\",\n",
            "      \"GPE\"\n",
            "    ],\n",
            "    [\n",
            "      \"Docker\",\n",
            "      \"PERSON\"\n",
            "    ],\n",
            "    [\n",
            "      \"AWS\",\n",
            "      \"ORG\"\n",
            "    ],\n",
            "    [\n",
            "      \"Kubernetes\",\n",
            "      \"ORG\"\n",
            "    ]\n",
            "  ],\n",
            "  \"lemmatized_resume_tokens_preview\": [\n",
            "    \"experienced\",\n",
            "    \"backend\",\n",
            "    \"engineer\",\n",
            "    \"build\",\n",
            "    \"restful\",\n",
            "    \"api\",\n",
            "    \"flask\",\n",
            "    \"django\",\n",
            "    \"work\",\n",
            "    \"postgresql\",\n",
            "    \"mongodb\",\n",
            "    \"familiar\",\n",
            "    \"docker\",\n",
            "    \"aws\",\n",
            "    \"good\",\n",
            "    \"knowledge\",\n",
            "    \"python\",\n",
            "    \"unit\",\n",
            "    \"testing\",\n",
            "    \"git\"\n",
            "  ],\n",
            "  \"lemmatized_jd_tokens_preview\": [\n",
            "    \"hire\",\n",
            "    \"backend\",\n",
            "    \"developer\",\n",
            "    \"python\",\n",
            "    \"experience\",\n",
            "    \"flask\",\n",
            "    \"fastapi\",\n",
            "    \"postgresql\",\n",
            "    \"mysql\",\n",
            "    \"containerization\",\n",
            "    \"docker\",\n",
            "    \"cloud\",\n",
            "    \"experience\",\n",
            "    \"aws\",\n",
            "    \"prefer\",\n",
            "    \"experience\",\n",
            "    \"kubernetes\",\n",
            "    \"plus\"\n",
            "  ]\n",
            "}\n",
            "Saved: sample_resume1_vs_jd1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9 — batch processing (assumes .txt files in folders ./resumes and ./jds)\n",
        "import os, glob\n",
        "\n",
        "def batch_match(resume_dir=\"./resumes\", jd_dir=\"./jds\", out_dir=\"./results\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    resume_files = sorted(glob.glob(os.path.join(resume_dir, \"*.txt\")))\n",
        "    jd_files = sorted(glob.glob(os.path.join(jd_dir, \"*.txt\")))\n",
        "\n",
        "    for rfile in resume_files:\n",
        "        rtext = open(rfile, encoding=\"utf-8\").read()\n",
        "        for jfile in jd_files:\n",
        "            jtext = open(jfile, encoding=\"utf-8\").read()\n",
        "            result = match_resume_jd(rtext, jtext)\n",
        "            outname = os.path.join(out_dir, f\"{os.path.basename(rfile)}__{os.path.basename(jfile)}.json\")\n",
        "            save_result_json(result, outname)\n",
        "\n",
        "# Example: create ./resumes and ./jds and run:\n",
        "# batch_match()\n"
      ],
      "metadata": {
        "id": "r9RCw3RvOW36"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_resume_jd(resume_text, jd_text):\n",
        "    resume_tokens = set(preprocess_text(resume_text))\n",
        "    jd_tokens = set(preprocess_text(jd_text))\n",
        "    overlap = resume_tokens.intersection(jd_tokens)\n",
        "    return {\n",
        "        \"matched_skills\": list(overlap),\n",
        "        \"score\": len(overlap)\n",
        "    }\n",
        "\n",
        "def save_result_json(result, filepath, resume_name=None, jd_name=None):\n",
        "    if resume_name: result[\"resume\"] = resume_name\n",
        "    if jd_name: result[\"job\"] = jd_name\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, indent=4)\n"
      ],
      "metadata": {
        "id": "n59Au6iPOW2w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_match(resume_dir=\"./resumes\", jd_dir=\"./jds\", out_dir=\"./results\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    resume_files = sorted(glob.glob(os.path.join(resume_dir, \"*.txt\")))\n",
        "    jd_files = sorted(glob.glob(os.path.join(jd_dir, \"*.txt\")))\n",
        "\n",
        "    for rfile in resume_files:\n",
        "        rtext = open(rfile, encoding=\"utf-8\").read()\n",
        "        for jfile in jd_files:\n",
        "            jtext = open(jfile, encoding=\"utf-8\").read()\n",
        "            result = match_resume_jd(rtext, jtext)\n",
        "            outname = os.path.join(out_dir, f\"{os.path.basename(rfile)}__{os.path.basename(jfile)}.json\")\n",
        "            save_result_json(result, outname, resume_name=os.path.basename(rfile), jd_name=os.path.basename(jfile))\n",
        "\n",
        "# Example: make folders ./resumes and ./jds, drop .txt files inside, then run:\n",
        "# batch_match()\n"
      ],
      "metadata": {
        "id": "M9yrRKm1Pg1s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample resumes and job descriptions (test cases)\n",
        "resumes = [\n",
        "    \"I am a software engineer skilled in Python, JavaScript, and data analysis. I have experience with machine learning and cloud computing.\",\n",
        "    \"Experienced front-end developer with expertise in React, HTML, CSS, and modern UI design. Worked on multiple web applications.\",\n",
        "    \"Data scientist with knowledge in SQL, Python, TensorFlow, and natural language processing. Strong background in statistics.\"\n",
        "]\n",
        "\n",
        "job_descriptions = [\n",
        "    \"We are looking for a Python developer with experience in machine learning and cloud technologies.\",\n",
        "    \"Hiring a front-end engineer skilled in React, CSS, and JavaScript to build interactive web applications.\",\n",
        "    \"Seeking a data scientist with SQL, NLP, and deep learning experience.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "cwvUhleVPxYl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated batch matching with our Day 2 test cases\n",
        "for i, resume in enumerate(resumes):\n",
        "    for j, jd in enumerate(job_descriptions):\n",
        "        result = match_resume_jd(resume, jd)\n",
        "        result[\"resume\"] = f\"resume_{i+1}.txt\"\n",
        "        result[\"job\"] = f\"job_{j+1}.txt\"\n",
        "        print(json.dumps(result, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "elgTL-68Pkc3",
        "outputId": "195ba093-f78d-4549-dff4-658ee4ba87d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4112570367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresumes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_resume_jd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"resume_{i+1}.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"job_{j+1}.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2315191550.py\u001b[0m in \u001b[0;36mmatch_resume_jd\u001b[0;34m(resume_text, jd_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_resume_jd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresume_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mjd_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
          ]
        }
      ]
    }
  ]
}